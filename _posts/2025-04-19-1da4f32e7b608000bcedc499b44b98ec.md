---
layout: post
title: "허수: LLM에서 꽃을 피다"
date: 2025-04-19 09:55:00 +0000
author: 정하성
categories: [Tech]
tags: [complex-numbers, transformer, positional-encoding, rope, natural-language-processing, euler-formula, artificial-intelligence]
summary: "이 문서는 허수 i가 현대 인공지능 기술, 특히 트랜스포머 모델에서 어떻게 중요한 역할을 하는지를 설명합니다. 트랜스포머는 병렬 처리로 뛰어난 성능을 보이지만, 문장 순서를 이해하는 데 한계가 있어 위치 인코딩을 사용합니다. 여기서 사인과 코사인 함수가 활용되며, 이는 복소수 지수 함수 e^{iθ}와 관련이 있습니다. 이러한 수학적 개념은 인공지능이 언어의 의미와 순서를 이해하는 데 도움을 주며, RoPE와 같은 기법으로 발전되어 대규모 언어 모델에서 효과적으로 사용됩니다."
---

![](https://haseong.github.io/assets/images/posts/1da4f32e7b608000bcedc499b44b98ec.png)

## **e^{iθ}****, 트랜스포머를 깨우다**

허수 i는 한때 “존재와 비존재 사이의 양서류”로 불리며 수학자들조차 그 유용성을 확신하지 못했던 개념이었습니다. 그러나 오일러의 공식과 가우스의 대수학적 완결성 증명이 보여주었듯, 허수는 전자공학의 전력 계산에서부터 양자역학의 확률 해석까지 현실을 움직이는 열쇠가 되었습니다. 상상의 수가 추상과 현실을 잇는 다리로 자리 잡는 데에는, 켤레복소수가 숨겨 준 대칭성과 회전의 언어가 결정적이었습니다. 이로써 우리는 “허수: 상상의 수가 현실을 움직이다”에서 허수가 어떻게 과학·공학 전반의 문제를 푸는 실용적 도구로 변모했는지 살펴보았습니다. 

하지만 허수의 여정은 여기서 끝나지 않습니다. 복소평면의 회전은 이제 거대한 언어 모델의 내부에서 다시 한 번 꽃을 피우고 있습니다. 트랜스포머가 문장의 순서를 이해하기 위해 도입한 위치 인코딩(Positional Embedding), 그리고 그것을 한 단계 확장한 RoPE(Rotary Positional Embedding)는 모두 e^{i\theta}의 주기성과 위상 차이를 빌려 단어 임베딩을 ‘회전’시키는 기법입니다. 사인과 코사인의 짝, 켤레복소수의 대칭 구조가 모델 내부에서 토큰 간 상대 위치를 계산하는 가장 경제적‧정교한 장치로 쓰이는 것이죠. 즉, 허수는 이제 전류나 파동을 넘어 언어 의미망의 질서를 조율하는 역할까지 맡게 된 것입니다. 

이번에는 허수가 어떻게 대규모 언어 모델의 심장부에 침투해 로터리 임베딩·푸리에 특징·상대 위치 인코딩 같은 기술로 구현되고 있는지를 탐구합니다. 추상 수학이 인공지능이라는 실전 무대에서 어떻게 다시 한 번 현실을 재구성하는지, 그리고 이 과정이 우리에게 어떤 새로운 통찰과 가능성을 열어 주는지를 살펴볼 것입니다. 1편이 허수의 역사와 실용성을 보여 주었다면, 2편은 허수가 디지털 시대에 어떻게 재해석되어 인간 언어를 이해하고 생성하는 ‘신경망의 나침반’으로 거듭났는지를 이야기합니다.

이제 복소평면의 회전이 거대한 언어 공간을 어떻게 안내하는지 함께 들여다보겠습니다.

# **트랜스포머와 순서 정보의 중요성**

자연어 처리 분야에서 혁신을 일으킨 트랜스포머(Transformer) 모델은 기존 순환 신경망(RNN)과 달리 한 번에 모든 단어를 병렬로 처리하여 뛰어난 성능을 보였습니다. 그러나 이러한 병렬 처리의 이면에는 치명적인 약점이 있습니다. 트랜스포머는 문장의 순서 정보를 스스로 알지 못한다는 점입니다. 예를 들어 "고양이가 개를 쫓았다"와 "개를 고양이가 쫓았다"는 단어 구성은 같지만 순서가 다르므로 뜻이 달라집니다. 트랜스포머 구조만으로는 이러한 차이를 인식할 수 없기 때문에, 각 단어의 위치 정보를 따로 모델에 알려줘야 합니다. 이를 위해 고안된 방법이 위치 인코딩(positional encoding)입니다. 위치 인코딩은 말 그대로 문장 내에서 각 단어의 위치를 고유한 방식으로 표현한 벡터를 의미하며, 이를 단어 임베딩에 더해줌으로써 모델이 단어의 상대적 순서를 파악할 수 있게 합니다.

# **사인과 코사인으로 표현하는 위치 인코딩**

트랜스포머 논문에서는 위치 인코딩을 구현하기 위해 놀랍게도 사인(sin) 함수와 코사인(cos) 함수를 활용했습니다. 각 단어의 위치를 하나의 각도로 보고, 그 각도의 사인값과 코사인값을 사용해 벡터 형태의 위치 표현을 만든 것입니다. 구체적으로는 입력 벡터 차원을 반으로 나누어, 절반은 다양한 주기의 사인 곡선 값으로, 나머지 절반은 동일한 주기의 코사인 값으로 채웁니다. 이렇게 하면 위치 p 에 대한 인코딩 벡터의 각 차원 2i와 2i+1에 대해 다음과 같은 형태를 띠게 됩니다:

- PE(p)_{2i} = \sin(\frac {p} {10000^{2i/d}})
- PE(p)_{2i+1} = \cos(\frac {p} {10000^{2i/d}})
여기서 d 는 임베딩 차원 수이고, i 는 0부터 시작하는 인덱스입니다. 이 수식이 다소 복잡해 보이지만, 요지는 여러 가지 주기(파장)를 갖는 사인파와 코사인파를 조합하여 각 위치마다 고유한 패턴의 값을 부여한다는 점입니다. 이를테면 어떤 차원에서는 비교적 빠르게 진동하는 짧은 주기의 사인 곡선으로 근처 위치 간 변화를 포착하고, 다른 차원에서는 아주 느리게 변화하는 긴 주기의 코사인 곡선으로 멀리 떨어진 위치 간의 차이를 구분하는 식입니다. 이렇게 사인과 코사인을 사용하면 모든 위치마다 서로 다른 값의 조합을 얻을 수 있고, 특히 일정 간격 떨어진 위치들 간에는 이러한 조합에서 일정한 규칙성이 나타납니다.

이러한 접근은 직관적으로 여러 개의 시계가 각기 다른 속도로 도는 모습에 비유할 수 있습니다. 각 위치 인코딩 차원이 하나의 시계바늘이라고 생각해보면, 어떤 바늘은 한 바퀴 도는 데 두 단어 간격이 걸리고 (짧은 주기), 또 다른 바늘은 만 단어를 지나야 한 바퀴 돌 정도로 느리게 움직입니다 (긴 주기). 문장 속에서 단어의 위치가 달라지면 이 여러 시계바늘들의 위치인 사인/코사인 값 조합도 달라지는데, 서로 다른 위치들은 결과적으로 서로 다른 시계 표시판을 가지게 됩니다.

# **오일러의 공식과 복소수 지수 함수**

사인과 코사인을 이용한 이러한 위치 인코딩 방식 뒤에는 오일러의 공식이라는 아름다운 수학이 숨어 있습니다. 오일러의 공식은 복소수 지수 함수와 삼각함수의 관계를 나타내는 공식으로 다음과 같습니다:

e^{i\theta} = \cos\theta + i\sin\theta

여기서 e 는 자연상수, i 는 상상의 단위, \theta 는 각도를 뜻합니다. 이 공식은 하나의 복소수 지수 e^{i\theta} 가 실수부로 \cos\theta를, 허수부로 \sin\theta 를 갖는다는 것을 의미합니다. 쉽게 말해, 복소평면에서 각도 \theta 만큼 회전한 단위벡터를 나타내는 것이 e^{i\theta} 이고, 이것을 펼치면 x축 방향 성분이 \cos\theta, y축 방향 성분이 \sin\theta 가 되는 셈입니다. 따라서 사인과 코사인은 복소수 지수 표현의 두 축을 담당하고 있다고 볼 수 있습니다. 트랜스포머의 위치 인코딩이 각 위치를 사인과 코사인 값 쌍으로 표현한 것도 결국은 복소수 e^{i\theta} 형태로 위치를 표현한 것과 같습니다. 단지 실제 구현에서는 복소수를 직접 쓰지 않고 실수 두 개로 쪼개어 표현했을 뿐입니다.

# **복소수 지수 표현이 위치 인코딩에 적합한 이유**

그렇다면 왜 하필 복소수 지수 함수를 이용한 표현이 위치 인코딩에 적합할까요? 주기성, 위상, 대칭성 측면에서 몇 가지 중요한 이점을 제공하기 때문입니다.

- 주기성: e^{i\theta}는 \theta 가 2\pi 증가할 때마다 같은 값으로 되돌아오는 주기성을 지닙니다. 즉 일정한 주기로 반복되는 패턴을 자연스럽게 표현할 수 있습니다. 이는 멀리 떨어진 위치라도 그 차이가 어떤 범위를 넘어가면 유사한 패턴으로 돌아올 수 있음을 시사합니다. 실제로 하나의 사인 혹은 코사인 함수만 쓸 경우에는 해당 주기 이상 떨어진 위치는 다시 비슷한 인코딩 값이 되겠지만, 다양한 주기의 함수를 함께 사용하면 서로 다른 주기의 주기성 패턴이 겹쳐 거의 모든 위치를 구별할 수 있습니다. 주기가 작은 성분은 미세한 차이를 구분하고, 주기가 큰 성분은 전체 범위에서의 거시적 위치를 가늠하게 해줍니다.
- 위상: 복소수 지수 표현의 핵심은 위상입니다. 두 개의 복소수 e^{i\theta_1}과 e^{i\theta_2}를 비교하면, 그 차이는 위상의 차이 \theta_1 - \theta_2로 나타납니다. 다시 말해, e^{i\theta} 형태로 위치를 표현하면 특정 위치 p 에서의 위상 각도와 또 다른 위치 q 에서의 위상 각도를 비교함으로써 p와 q 사이의 상대적 거리를 자연스럽게 얻어낼 수 있습니다. 이는 모델이 절대적인 위치 정보보다는 상대적인 순서와 간격에 집중할 수 있도록 돕습니다. 인간이 시간을 볼 때도 절대 시각보다 “몇 시간 차이”를 중요하게 여기듯이, 모델도 두 단어 사이의 거리 p-q 에 해당하는 정보에 쉽게 접근하는 셈입니다.
- 대칭성: e^{i\theta}와 그 켤레 복소수인 e^{-i\theta}는 서로 거울 대칭과 같은 관계에 있습니다. 이는 한편으로 \cos\theta 가  우함수인 점 “\cos(-\theta) = \cos\theta” 과 \sin\theta가  기함수인 “\sin(-\theta) = -\sin\theta” 과 연결됩니다. 이러한 성질 덕분에, 위치 인코딩에 사인과 코사인 쌍을 사용하면 순서의 정반대인 경우도 표현이 자연스럽게 대응됩니다. 예를 들어 순서가 반대로 된 문장을 입력하면 위치 인코딩 벡터도 마치 복소수 평면에서 대칭점을 보는 것처럼 변하게 됩니다. 결과적으로 모델은 입력 순서가 거꾸로인 경우에도 일정 부분 유사한 패턴임을 느낄 수 있고, 필요하다면 그 반대 관계 역시 학습으로 다룰 수 있게 됩니다.
또한 복소수 지수 표현은 크기가 1로 일정하다는 특징이 있습니다. |e^{i\theta}| = 1이므로 어떤 위치이든 간에 표현 벡터의 크기가 변하지 않습니다. 이는 위치가 커질수록 값이 발산하거나 하지 않음을 의미하며, 학습 과정에서 안정성을 제공합니다. 사인과 코사인값이 항상 -1과 1 사이에서 놀기 때문에, 단어의 위치 번호가 아무리 커져도 인코딩 벡터의 요소들이 폭주하지 않는 것이죠.

# **푸리에 변환 관점에서 본 위치 인코딩**

사인과 코사인으로 위치를 표현하는 방식은 푸리에 변환과 깊은 연관이 있습니다. 푸리에 변환은 임의의 신호나 함수를 주파수 성분으로 분해하는 기법으로, 어떤 신호도 충분히 많은 사인파와 코사인파의 합으로 표현될 수 있다는 통찰에서 출발합니다. 트랜스포머의 위치 인코딩을 이 관점에서 바라보면, 마치 시간 영역의 ‘짧은 신호’를 주파수 영역으로 표현한 것과 비슷합니다. 예를 들어, 문장 위치 p 에서 특정 단어가 하나 뚝 떨어져 있다고 상상하면, 이는 시간 신호 처리에서 디랙 델타 \delta(t-p)와 유사합니다. 이 델타 신호를 푸리에 변환하면 모든 주파수 성분을 동일하게 포함한 복소 지수 함수 e^{-i\omega p}가 나오는데, 이것을 몇 가지 대표 주파수에서만 샘플링하면 바로 사인/코사인 기반의 위치 인코딩이 됩니다. 달리 말해, 위치 인코딩 벡터는 해당 위치를 나타내는 주파수 스펙트럼 “진동 성분들의 조합” 이라고 볼 수 있습니다.

이렇게 얻어진 위치 인코딩은 신호처리적 관점에서 매우 의미 있는 표현입니다. 주파수 성분으로 표현되었기에, 두 위치 인코딩을 비교하면 둘 사이 위상차를 각 주파수별로 얻어낼 수 있습니다. 이는 마치 두 신호 사이의 시간 차이를 푸리에 도메인에서 알아채는 것과 같습니다. 실제로 트랜스포머의 설계자들은 이러한 주파수 기반 표현 덕분에, 위치 차이가 일정하다면 그 차이를 나타내는 패턴도 선형 변환으로 얻어질 것이라고 가설했습니다. 이 말은, 위치 인코딩 벡터들 사이에 특정 상대 거리 \phi 가 있다면 하나의 단순한 행렬 곱 “선형 변환” 으로도 p에서 p+\phi로의 이동을 표현할 수 있다는 뜻입니다. 신호의 주파수 성분이 가지는 선형성과 주기성이 이러한 상대 위치 인지 용이성을 뒷받침해주는 셈입니다.

# **복소수 곱셈과 켤레를 통한 상대 위치 추출**

복소수 지수 형태로 위치를 표현하면 복소수 곱을 통해 쉽게 두 위치의 차이를 계산할 수 있습니다. 예를 들어 위치 m에서의 인코딩을 e^{i\theta m}으로, 위치 n에서의 인코딩을 e^{i\theta n}으로 놓아봅시다. 이 둘을 곱하면:

e^{i\theta m} \times e^{i\theta n} = e^{i\theta (m+n)}

가 되어 위치값이 합산됩니다. 하지만 우리가 알고 싶은 것은 두 위치의 차이입니다. 한쪽에 켤레 복소수를 취해 곱하면 어떻게 될까요? e^{i\theta m} \times e^{-i\theta n} = e^{i\theta (m-n)} 처럼 지수에서 m-n이 나타납니다. 이는 복소수 표현된 두 위치를 비교할 때, 하나를 켤레로 뒤집은 다음 곱하면 곧바로 상대적인 거리 m-n에 해당하는 위상 성분을 얻을 수 있다는 의미입니다.

트랜스포머의 어텐션 메커니즘에서 두 단어의 유사도를 측정하는 핵심 연산은 내적(dot product)입니다. 위치 인코딩을 도입하면 이 내적 계산에 위상차 정보가 스며들게 됩니다. 간단한 사례로, 2차원으로 표현된 벡터를 f(x, m) = x \cdot e^{i \theta m} 형태로 위치 m에 따라 회전시키고, 다른 위치 n에 있는 벡터 f(x, n) = x \cdot e^{i \theta n}와 내적한다고 해봅시다. 이 점곱 값은 결국 x 자체의 내용 유사도와 함께 e^{i\theta m}과 e^{-i\theta n}의 곱에 해당하는 위상차 \theta(m-n)의 코사인값으로 표현됩니다. 직관적으로 보면, 두 벡터의 내용이 같더라도 위치 차이가 크면 위상차도 커져 코사인값(유사도)이 낮아지고, 위치가 비슷하면 위상차가 작아 코사인값이 높아지는 식입니다. 이렇게 복소수 곱과 켤레를 활용하면 모델이 관심 가져야 할 것은 벡터들의 상대적 위치뿐이 되고, 절대적인 위치 번호 자체는 수식에서 소거됩니다. 이것이 앞서 설명한 대로 위치 인코딩이 상대적인 순서 정보에 모델의 주의를 돌릴 수 있게 하는 원리입니다.

# **대규모 언어 모델에서의 활용: RoPE 기법**

오늘날의 대규모 언어 모델(LLM)들, 예를 들어 GPT 계열 모델이나 Meta의 LLaMA 등에서도 이러한 복소수 기반 위치 인코딩 아이디어가 다양하게 활용되고 있습니다. 그중 한 가지 발전된 방식이 RoPE(Rotary Positional Embedding)라는 기법입니다. RoPE에서는 앞서 설명한 사인/코사인 위치 인코딩을 단순히 더하는 대신, 토큰 임베딩 자체를 복소수 평면에서 회전시키는 접근을 취합니다. 이는 벡터의 각 차원 쌍을 복소수의 실수부와 허수부로 간주하여, 해당 벡터를 위치에 따라 일정 각도만큼 회전시키는 것입니다. 쉽게 말해, 임베딩 벡터를 위치에 비례하는 위상 각도로 돌려버리는 것입니다.

예를 들어 임베딩 벡터의 한 쌍의 차원이 (x_{\text{real}}, x_{\text{imag}})라면, 위치 p 에서 이를 \theta \times p 만큼 회전시켜 새로운 값 (x_{\text{real}} \cos(\theta p) - x_{\text{imag}} \sin(\theta p), x_{\text{real}} \sin(\theta p) + x_{\text{imag}} \cos(\theta p))로 바꾸는 식입니다. 각 차원 쌍마다 서로 다른 회전 속도 “주파수 \theta 값”을 부여하여, 전통적인 사인/코사인 위치 인코딩과 동일한 정보를 내재하지만 연산은 임베딩을 직접 변환하는 형태가 됩니다. 이렇게 하면 어텐션 계산 단계에서 두 벡터의 내적을 수행할 때 자동으로 위치 차이에 따른 위상차가 반영되도록 만들 수 있습니다. 결과적으로 모델이 상대적 위치 관계를 더 직접적으로 이해할 수 있게 해주며, 추가적인 파라미터 없이 순수하게 수학적 성질만으로 위치 정보를 활용한다는 장점이 있습니다.

RoPE 기법은 실제로 현재 여러 언어 모델에서 사용되고 있으며, 특히 문맥 길이가 긴 입력에서도 효율적으로 상대적 위치를 처리할 수 있게 해준다는 평가를 받습니다. 이러한 발전은 복소수 e^{i\theta}의 힘을 빌려, 모델이 단순히 단어들의 의미뿐만 아니라 그 배열 순서의 미묘한 차이까지 잘 포착하도록 돕고 있습니다.

# **허수가 인공지능에 주는 함의**

한때 “상상의 수”라 불리며 현실에 존재하지 않는 숫자로 여겨졌던 허수 i가, 현대 인공지능 기술의 핵심인 트랜스포머 모델 속에서 중요한 역할을 하고 있다는 사실은 우리에게 많은 생각거리를 제공합니다. 복잡하고 추상적인 수학 개념인 허수와 복소수가, 언어를 이해하고 생성하는 거대 신경망의 내부 표현으로 쓰이고 있다는 점은 그 자체로 놀라운 일입니다. 이는 순수한 수학적 직관이 어떻게 실용적인 기술로 연결되는가를 보여주는 생생한 사례이기도 합니다. 수 세기 전 오일러는 상상 속 개념이었던 e^{i\theta} 로부터 우주 속 질서와 대칭성을 통찰했는데, 오늘날 우리는 그 동일한 공식을 활용하여 기계가 언어의 질서를 이해하도록 만들고 있습니다.

요컨대, 허수는 더 이상 “상상의 산물”에 머무르지 않고 현실의 데이터를 움직이는 도구가 되었습니다. 인간의 언어를 배우는 기계의 눈에 비친 세계는 복소수 평면 위의 회전과 패턴으로 가득하며, 이 추상적인 회전의 언어가 곧 의미의 차이를 만들어냅니다. 이는 곧 수학과 인공지능의 아름다운 연결이라고 할 수 있습니다. 우리가 상상으로만 그리던 개념이 현실의 거대 언어 모델 속에서 작동하며, 상상의 수 i 가 현실 세계의 문제를 풀어가는 데 일조하는 모습은, 과학과 수학의 경이로움을 다시금 일깨워줍니다. 인공지능 기술 속에 스며든 허수 개념은 보이지 않는 곳에서 현실을 움직이는 힘으로서, 추상과 현실을 이어주는 철학적 다리 역할을 하고 있는 것입니다.

## 참고자료

[Transformer Architecture: The Positional Encoding – Amirhossein Kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/?utm_source=chatgpt.com)

[Why Are Sines and Cosines Used For Positional Encoding? – Faizan M.](https://mfaizan.github.io/2023/04/02/sines.html?utm_source=chatgpt.com)

[Fourier Feature Encoding – Synerise AI](https://sair.synerise.com/fourier-feature-encoding/?utm_source=chatgpt.com)

[A Deep Dive into Rotary Positional Embeddings (RoPE) – Parul Sharma](https://medium.com/%40parulsharmmaa/understanding-rotary-positional-embedding-and-implementation-9f4ad8b03e32?utm_source=chatgpt.com)

[Positional Encoding in Transformers – GeeksforGeeks](https://www.geeksforgeeks.org/positional-encoding-in-transformers/)



